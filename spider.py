import requests
import time
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.action_chains import ActionChains
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.chrome.service import Service
import time


HEADERS = {"User-Agent": "Mozilla/5.0", "Referer": "https://www.cls.cn/"}

DETAIL_URL = "https://www.cls.cn/detail/{}"
LIST_API = "https://www.cls.cn/v3/depth/home/assembled/1032"


def get_all_detail_ids():
    options = webdriver.ChromeOptions()
    options.add_argument("--disable-blink-features=AutomationControlled")

    driver = webdriver.Chrome(
        service=Service(ChromeDriverManager().install()), options=options
    )

    driver.get("https://www.cls.cn/depth?id=1032")
    time.sleep(5)

    ids = set()

    for _ in range(20):  # åŠ è½½æ›´å¤šæ¬¡æ•°ï¼Œå¯è‡ªè¡Œè°ƒ
        links = driver.find_elements(By.CSS_SELECTOR, "a[href^='/detail/']")
        for a in links:
            href = a.get_attribute("href")
            if href and "/detail/" in href:
                ids.add(int(href.split("/")[-1]))

        # æ»šåŠ¨è§¦å‘åŠ è½½
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(2)

        try:
            btn = driver.find_element(By.XPATH, "//div[contains(text(),'åŠ è½½æ›´å¤š')]")
            btn.click()
            time.sleep(2)
        except:
            pass

    driver.quit()
    return list(ids)


def get_news_list_html():
    url = "https://www.cls.cn/depth?id=1032"
    r = requests.get(url, headers=HEADERS, timeout=10)
    r.raise_for_status()

    soup = BeautifulSoup(r.text, "html.parser")

    news = []
    for a in soup.select("a[href^='/detail/']"):
        href = a.get("href")
        if href:
            try:
                news_id = int(href.split("/")[-1])
                news.append(news_id)
            except:
                pass

    # å»é‡
    return list(set(news))


def parse_detail(news_id):
    """
    è§£æè¯¦æƒ…é¡µ
    """
    url = DETAIL_URL.format(news_id)
    r = requests.get(url, headers=HEADERS, timeout=10)
    r.raise_for_status()

    soup = BeautifulSoup(r.text, "html.parser")

    # æ ‡é¢˜
    title = soup.select_one("div.detail-title span")
    title = title.text.strip() if title else ""

    # æ‘˜è¦
    summary = soup.select_one("pre.detail-brief")
    summary = summary.text.strip() if summary else ""

    # å‘å¸ƒæ—¶é—´ & æ¥æº
    info = soup.select_one("div.c-999")
    publish_time, source = "", ""
    if info:
        spans = info.select("div.f-l")
        if len(spans) >= 2:
            publish_time = spans[1].text.strip()
        if len(spans) >= 3:
            source = spans[2].text.strip()

    # æ­£æ–‡ + å›¾ç‰‡
    content_div = soup.select_one("div.detail-content")
    paragraphs = []
    images = []

    if content_div:
        for p in content_div.find_all(["p", "h3"]):
            if p.name == "p":
                img = p.find("img")
                if img:
                    images.append(img["src"])
                else:
                    text = p.text.strip()
                    if text:
                        paragraphs.append(text)
            else:
                paragraphs.append(p.text.strip())

    content = "\n".join(paragraphs)

    return {
        "id": news_id,
        "title": title,
        "summary": summary,
        "content": content,
        "images": images,
        "publish_time": publish_time,
        "source": source,
        "url": url,
    }


def crawl():
    news_ids = get_all_detail_ids()
    print(f"ğŸ“„ å…±å‘ç° {len(news_ids)} æ¡æ–°é—»")

    results = []
    for news_id in news_ids:
        try:
            data = parse_detail(news_id)
            results.append(data)
            print(f"âœ… {data['title']}")
            time.sleep(1)
        except Exception as e:
            print(f"âŒ {news_id} å¤±è´¥ï¼š{e}")

    return results


if __name__ == "__main__":
    data = crawl()
    print(f"\nå…±æŠ“å– {len(data)} æ¡æ–°é—»")
